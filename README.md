1. Introduction
This project aims to build a regression model using a transformer-based architecture leveraging BERT (Bidirectional Encoder Representations from Transformers). The goal is to predict a target value based on input text sequences.

2. Model Architecture
The model is based on the bert-base-uncased pre-trained model from Hugging Face's transformers library. The architecture includes:

BERT Model: Extracts contextualized embeddings from input text.
Dropout Layer: Adds regularization to prevent overf
